{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI ML Repository Character Font Images for MNIST Deep Learning Digit Recognition\n",
    "\n",
    "[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) provides a variety of free datasets for practicing machine leanring. In this project, I will be using data from [Character Font Images](https://archive.ics.uci.edu/ml/datasets/Character+Font+Images) to perform digit recognition tasks.\n",
    "\n",
    "The *Character Font Images* dataset contains 745000 images from 153 character fonts. The size of each image is 20x20 in grey scale (intensity 1 to 255, while 0 is white and 255 is black). Each data file is a CSV file with the following information:\n",
    "\n",
    "| Field | Type | Unique | Example | Description |\n",
    "|:-:|:----:|:------:|:-------:|:---:|\n",
    "|font  | string | 153 | times | font family|\n",
    "|fontVariant | string | 248 | times new roman | If the font image was from a scanner, the fontVariant is scanned. otherwise it is the font name. |\n",
    "|m_label | integer | 11597 | 33 to 65535 | The character value, for instance 48 for the digit, 0 |\n",
    "|strength | real | 2 |.4 | A value 0 to 1, indicating normal or bold |\n",
    "|italic | integer | 2 | 1 | A flag, if 1, the image was computer generated with the an italic font. |\n",
    "|m_top | integer | 13 |  | The topmost black pixel row index in the original image from which the image was cut |\n",
    "|m_left | integer | 43 |  | The leftmost black pixel column index in the original image from which the image was cut| \n",
    "|originalH | integer | 30 |  | The original height of the image in pixels |\n",
    "|originalW | integer| 36 |  | The original width of the image in pixels |\n",
    "|h | integer | 1 | 20 | The image height in this sample, always 20 |\n",
    "|w | integer | 1 | 20 | The image width in this sample, always 20 |\n",
    "|r0c0 | integer | 255 | 0 | Row 0 Column 0 pixel value, 0 to 255, white is 0, 255 is black |\n",
    "|r0c1 | integer | 255 | 0 | Row 0, Column 1 pixel value, 0 to 255 |\n",
    "|r19c19 | integer | 255 |  | Row 19, Column 19 pixel value, 0 to 255|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "The task is to use a variety of machine learning algorithms to build digit (0-9) recognition models using images from this dataset. As with most machine learning tasks, the todo list is as follows:\n",
    "\n",
    "1. Importing and parsing the datasets\n",
    "2. Descriptives and exploratory data analysis\n",
    "3. Select a type of model\n",
    "4. Train the model\n",
    "5. Evaluate the model's effectiveness\n",
    "6. Use the trained model to make predictions\n",
    "7. Repeat steps 3-6 with increased model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 and 2: Importing and cleaning the data, explore data\n",
    "\n",
    "The dataset consists of 153 csv files, each with columns that are useful, and columns that are not. The first step is to load a few individual files and inspect them. Then, we need to extract rows that are images of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir('./data')\n",
    "#print(files)\n",
    "print(len(files)) # confirm that there are 153 csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "font            object\n",
      "fontVariant     object\n",
      "m_label          int64\n",
      "strength       float64\n",
      "italic           int64\n",
      "orientation    float64\n",
      "m_top            int64\n",
      "m_left           int64\n",
      "originalH        int64\n",
      "originalW        int64\n",
      "h                int64\n",
      "w                int64\n",
      "r0c0             int64\n",
      "r0c1             int64\n",
      "r0c2             int64\n",
      "r0c3             int64\n",
      "r0c4             int64\n",
      "r0c5             int64\n",
      "r0c6             int64\n",
      "r0c7             int64\n",
      "r0c8             int64\n",
      "r0c9             int64\n",
      "r0c10            int64\n",
      "r0c11            int64\n",
      "r0c12            int64\n",
      "r0c13            int64\n",
      "r0c14            int64\n",
      "r0c15            int64\n",
      "r0c16            int64\n",
      "r0c17            int64\n",
      "                ...   \n",
      "r18c10           int64\n",
      "r18c11           int64\n",
      "r18c12           int64\n",
      "r18c13           int64\n",
      "r18c14           int64\n",
      "r18c15           int64\n",
      "r18c16           int64\n",
      "r18c17           int64\n",
      "r18c18           int64\n",
      "r18c19           int64\n",
      "r19c0            int64\n",
      "r19c1            int64\n",
      "r19c2            int64\n",
      "r19c3            int64\n",
      "r19c4            int64\n",
      "r19c5            int64\n",
      "r19c6            int64\n",
      "r19c7            int64\n",
      "r19c8            int64\n",
      "r19c9            int64\n",
      "r19c10           int64\n",
      "r19c11           int64\n",
      "r19c12           int64\n",
      "r19c13           int64\n",
      "r19c14           int64\n",
      "r19c15           int64\n",
      "r19c16           int64\n",
      "r19c17           int64\n",
      "r19c18           int64\n",
      "r19c19           int64\n",
      "Length: 412, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>font</th>\n",
       "      <th>fontVariant</th>\n",
       "      <th>m_label</th>\n",
       "      <th>strength</th>\n",
       "      <th>italic</th>\n",
       "      <th>orientation</th>\n",
       "      <th>m_top</th>\n",
       "      <th>m_left</th>\n",
       "      <th>originalH</th>\n",
       "      <th>originalW</th>\n",
       "      <th>...</th>\n",
       "      <th>r19c10</th>\n",
       "      <th>r19c11</th>\n",
       "      <th>r19c12</th>\n",
       "      <th>r19c13</th>\n",
       "      <th>r19c14</th>\n",
       "      <th>r19c15</th>\n",
       "      <th>r19c16</th>\n",
       "      <th>r19c17</th>\n",
       "      <th>r19c18</th>\n",
       "      <th>r19c19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>64258</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>64257</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>61442</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>61441</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>9674</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>8805</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>8804</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>8800</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>8776</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AGENCY</td>\n",
       "      <td>AGENCY FB</td>\n",
       "      <td>8747</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>176</td>\n",
       "      <td>116</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 412 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     font fontVariant  m_label  strength  italic  orientation  m_top  m_left  \\\n",
       "0  AGENCY   AGENCY FB    64258       0.4       0          0.0     35      21   \n",
       "1  AGENCY   AGENCY FB    64257       0.4       0          0.0     35      21   \n",
       "2  AGENCY   AGENCY FB    61442       0.4       0          0.0     35      21   \n",
       "3  AGENCY   AGENCY FB    61441       0.4       0          0.0     35      21   \n",
       "4  AGENCY   AGENCY FB     9674       0.4       0          0.0     51      21   \n",
       "5  AGENCY   AGENCY FB     8805       0.4       0          0.0     51      23   \n",
       "6  AGENCY   AGENCY FB     8804       0.4       0          0.0     51      22   \n",
       "7  AGENCY   AGENCY FB     8800       0.4       0          0.0     47      23   \n",
       "8  AGENCY   AGENCY FB     8776       0.4       0          0.0     53      26   \n",
       "9  AGENCY   AGENCY FB     8747       0.4       0          0.0     35      19   \n",
       "\n",
       "   originalH  originalW   ...    r19c10  r19c11  r19c12  r19c13  r19c14  \\\n",
       "0         51         22   ...         1       1       1       1       1   \n",
       "1         51         22   ...         1       1       1       1       1   \n",
       "2         51         22   ...         1       1       1       1       1   \n",
       "3         51         22   ...         1       1       1       1       1   \n",
       "4         33         25   ...       255     132       1       1       1   \n",
       "5         30         17   ...       255     255     255     255     255   \n",
       "6         30         17   ...       255     255     255     255     255   \n",
       "7         37         23   ...         1       1       1       1       1   \n",
       "8         22         23   ...         1       1      20      24      74   \n",
       "9         64         16   ...       176     116      48       0       0   \n",
       "\n",
       "   r19c15  r19c16  r19c17  r19c18  r19c19  \n",
       "0       1     163     255     255     255  \n",
       "1       1     163     255     255     255  \n",
       "2       1     163     255     255     255  \n",
       "3       1     163     255     255     255  \n",
       "4       1       1       1       1       1  \n",
       "5     255     255     255     255     255  \n",
       "6     255     255     255     255     255  \n",
       "7       1       1       1       1       1  \n",
       "8     255     255     255     255      54  \n",
       "9       0       0       0       0       0  \n",
       "\n",
       "[10 rows x 412 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file = pd.read_csv('./data/' + files[0], index_col=False)\n",
    "print(first_file.dtypes)\n",
    "first_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file.isna().sum().sum() # checking that no cell is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some information that we definitely do not need here. The first two columns contain information about fonts, which is not necessary here. Whether the character is bold or italic might be useful in some models, which I will keep. `m_label` is the label here which I will keep. Next step will be extracting only images that are numbers. If each `m_label` value is unique, we will be able to use it as the row index, and extracting those images would be much easy. Unfortunately, the following validation indicates that it's not. We will need to use a numeric range for choosing all digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n",
      "251\n"
     ]
    }
   ],
   "source": [
    "print(first_file.shape[0])\n",
    "print(len(first_file.m_label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 412)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file_digits = first_file[first_file.m_label.apply(lambda x: x >= ord('0') and x <= ord('9'))]\n",
    "first_file_digits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show one of the images:\n",
    "\n",
    "def show_image(df, row):\n",
    "    from PIL import Image\n",
    "    array = np.asarray(df.iloc[row, -400:]).astype(np.uint8)\n",
    "    img = Image.frombytes('L', (20, 20), array)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "show_image(first_file_digits, 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to batch import data from all CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENCY.csv (40, 403)\n",
      "ARIAL.csv (7844, 403)\n",
      "BAITI.csv (40, 403)\n",
      "BANKGOTHIC.csv (80, 403)\n",
      "BASKERVILLE.csv (40, 403)\n",
      "BAUHAUS.csv (40, 403)\n",
      "BELL.csv (40, 403)\n",
      "BERLIN.csv (80, 403)\n",
      "BERNARD.csv (40, 403)\n",
      "BITSTREAMVERA.csv (596, 403)\n",
      "BLACKADDER.csv (40, 403)\n",
      "BODONI.csv (160, 403)\n",
      "BOOK.csv (40, 403)\n",
      "BOOKMAN.csv (40, 403)\n",
      "BRADLEY.csv (40, 403)\n",
      "BRITANNIC.csv (40, 403)\n",
      "BROADWAY.csv (40, 403)\n",
      "BRUSH.csv (40, 403)\n",
      "BUXTON.csv (40, 403)\n",
      "CAARD.csv (80, 403)\n",
      "CALIBRI.csv (80, 403)\n",
      "CALIFORNIAN.csv (40, 403)\n",
      "CALISTO.csv (40, 403)\n",
      "CAMBRIA.csv (40, 403)\n",
      "CANDARA.csv (40, 403)\n",
      "CASTELLAR.csv (40, 403)\n",
      "CENTAUR.csv (40, 403)\n",
      "CENTURY.csv (120, 403)\n",
      "CHILLER.csv (40, 403)\n",
      "CITYBLUEPRINT.csv (40, 403)\n",
      "COMIC.csv (40, 403)\n",
      "COMMERCIALSCRIPT.csv (40, 403)\n",
      "COMPLEX.csv (40, 403)\n",
      "CONSOLAS.csv (40, 403)\n",
      "CONSTANTIA.csv (40, 403)\n",
      "COOPER.csv (40, 403)\n",
      "COPPERPLATE.csv (80, 403)\n",
      "CORBEL.csv (40, 403)\n",
      "COUNTRYBLUEPRINT.csv (40, 403)\n",
      "COURIER.csv (573, 403)\n",
      "CREDITCARD.csv (29449, 403)\n",
      "CURLZ.csv (40, 403)\n",
      "DUTCH801.csv (80, 403)\n",
      "E13B.csv (23311, 403)\n",
      "EBRIMA.csv (40, 403)\n",
      "EDWARDIAN.csv (40, 403)\n",
      "ELEPHANT.csv (40, 403)\n",
      "ENGLISH.csv (40, 403)\n",
      "ENGRAVERS.csv (40, 403)\n",
      "ERAS.csv (160, 403)\n",
      "EUROROMAN.csv (40, 403)\n",
      "FELIX TITLING.csv (40, 403)\n",
      "FOOTLIGHT.csv (40, 403)\n",
      "FORTE.csv (40, 403)\n",
      "FRANKLIN.csv (240, 403)\n",
      "FREESTYLE.csv (40, 403)\n",
      "FRENCH.csv (40, 403)\n",
      "GABRIOLA.csv (40, 403)\n",
      "GADUGI.csv (40, 403)\n",
      "GARAMOND.csv (40, 403)\n",
      "GEORGIA.csv (40, 403)\n",
      "GIGI.csv (40, 403)\n",
      "GILL.csv (200, 403)\n",
      "GLOUCESTER.csv (40, 403)\n",
      "GOTHICE.csv (40, 403)\n",
      "GOUDY.csv (80, 403)\n",
      "GUNPLAY.csv (40, 403)\n",
      "HAETTENSCHWEILER.csv (40, 403)\n",
      "HANDPRINT.csv (70000, 403)\n",
      "HARLOW.csv (40, 403)\n",
      "HARRINGTON.csv (40, 403)\n",
      "HIGH TOWER.csv (40, 403)\n",
      "HIMALAYA.csv (40, 403)\n",
      "IMPACT.csv (40, 403)\n",
      "IMPRINT.csv (40, 403)\n",
      "INFORMAL.csv (40, 403)\n",
      "ISOC.csv (280, 403)\n",
      "ITALIC.csv (120, 403)\n",
      "JAVANESE.csv (40, 403)\n",
      "JOKERMAN.csv (40, 403)\n",
      "JUICE.csv (40, 403)\n",
      "KRISTEN.csv (40, 403)\n",
      "KUNSTLER.csv (40, 403)\n",
      "LEELAWADEE.csv (80, 403)\n",
      "LUCIDA.csv (320, 403)\n",
      "MAGNETO.csv (40, 403)\n",
      "MAIANDRA.csv (40, 403)\n",
      "MATURA.csv (40, 403)\n",
      "MINGLIU.csv (80, 403)\n",
      "MISTRAL.csv (40, 403)\n",
      "MODERN.csv (40, 403)\n",
      "MONEY.csv (5784, 403)\n",
      "MONOSPAC821.csv (40, 403)\n",
      "MONOTXT.csv (40, 403)\n",
      "MONOTYPE.csv (40, 403)\n",
      "MV_BOLI.csv (40, 403)\n",
      "MYANMAR.csv (40, 403)\n",
      "NIAGARA.csv (80, 403)\n",
      "NINA.csv (40, 403)\n",
      "NIRMALA.csv (80, 403)\n",
      "NUMERICS.csv (13704, 403)\n",
      "OCRA.csv (19323, 403)\n",
      "OCRB.csv (35879, 403)\n",
      "ONYX.csv (40, 403)\n",
      "PALACE.csv (40, 403)\n",
      "PALATINO.csv (40, 403)\n",
      "PANROMAN.csv (40, 403)\n",
      "PAPYRUS.csv (40, 403)\n",
      "PERPETUA.csv (80, 403)\n",
      "PHAGSPA.csv (40, 403)\n",
      "PLAYBILL.csv (40, 403)\n",
      "PMINGLIU-EXTB.csv (40, 403)\n",
      "PRISTINA.csv (40, 403)\n",
      "PROXY.csv (360, 403)\n",
      "QUICKTYPE.csv (120, 403)\n",
      "RAGE.csv (40, 403)\n",
      "RAVIE.csv (40, 403)\n",
      "REFERENCE.csv (40, 403)\n",
      "RICHARD.csv (40, 403)\n",
      "ROCKWELL.csv (120, 403)\n",
      "ROMAN.csv (160, 403)\n",
      "ROMANTIC.csv (40, 403)\n",
      "SANSSERIF.csv (40, 403)\n",
      "SCRIPT.csv (80, 403)\n",
      "SCRIPTB.csv (40, 403)\n",
      "SEGOE.csv (440, 403)\n",
      "SERIF.csv (40, 403)\n",
      "SHOWCARD.csv (40, 403)\n",
      "SIMPLEX.csv (40, 403)\n",
      "SITKA.csv (240, 403)\n",
      "SKETCHFLOW.csv (40, 403)\n",
      "SNAP.csv (40, 403)\n",
      "STENCIL.csv (40, 403)\n",
      "STYLUS.csv (40, 403)\n",
      "SUPERFRENCH.csv (40, 403)\n",
      "SWIS721.csv (480, 403)\n",
      "SYLFAEN.csv (40, 403)\n",
      "TAHOMA.csv (40, 403)\n",
      "TAI.csv (80, 403)\n",
      "TECHNIC.csv (120, 403)\n",
      "TEMPUS.csv (40, 403)\n",
      "TIMES.csv (1258, 403)\n",
      "TREBUCHET.csv (40, 403)\n",
      "TW.csv (120, 403)\n",
      "TXT.csv (40, 403)\n",
      "VERDANA.csv (40, 403)\n",
      "VIN.csv (638, 403)\n",
      "VINER.csv (40, 403)\n",
      "VINETA.csv (40, 403)\n",
      "VIVALDI.csv (40, 403)\n",
      "VLADIMIR.csv (40, 403)\n",
      "WIDE.csv (40, 403)\n",
      "YI BAITI.csv (40, 403)\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv('./data/' + file, index_col=False)\n",
    "    df = df[df.m_label.apply(lambda x: x >= ord('0') and x <= ord('9'))]\n",
    "    df.drop(columns = first_file.columns[[0, 1, 5, 6, 7, 8, 9, 10, 11]], inplace=True)\n",
    "    print(file, df.shape)\n",
    "    df_list.append(df)\n",
    "    \n",
    "df_digits = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217639, 403)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_digits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have two more problems: 1) `m_label` needs to be converted back to 0 to 9, and 2) the column `strength` is pretty confusing. We will need to change its name to `\"bold\"` and give it binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_digits.m_label = df_digits.m_label - 48\n",
    "bold = df_digits.strength == 0.7\n",
    "df_digits.drop(columns = [\"strength\"], inplace=True)\n",
    "df_digits.insert(1, \"bold\", bold.astype(\"int64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m_label\n",
       "0    28034\n",
       "1    28879\n",
       "2    22170\n",
       "3    20431\n",
       "4    21233\n",
       "5    18444\n",
       "6    22360\n",
       "7    18091\n",
       "8    20669\n",
       "9    17328\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_digits.groupby(\"m_label\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataset has 217639 images and 10 labels, and with the exception of 0s and 1s, these labels have similar numbers of examples. Since this dataset is big enough, I will use 70% of this dataset for training, 15% for validation, 15% for testing. **n-fold validation** is computationally intensive and really not necessary for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Select training models\n",
    "\n",
    "Since this is a classification task, I will use 3 different types of classification models. \n",
    "1. A \"vanilla\" model (multinomial regression)\n",
    "2. An ensemble model (eXtreme Gradient Boosting, XGBoost)\n",
    "3. A deep learning model (LeNet5 Convolutional Neural Network).\n",
    "\n",
    "The first two will require a bit of feature engineering, mainly to reduce the dimension of the data. I will\n",
    "\n",
    "1. Check if any column has low variance. If yes, remove them.\n",
    "2. Perform principal component analysis (PCA) to reduce dimensionality\n",
    "3. Split the dataset (70, 15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_digits.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27f81d78668>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stds = df_copy.apply(pd.Series.std, axis = 0)[3:]\n",
    "stds.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217639, 44)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "imgs = df_copy.iloc[:, 3:]\n",
    "pca = PCA(0.88)\n",
    "transformed = pca.fit_transform(imgs)\n",
    "transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis retained 88% of the total variance with 44 components. Now I will build a multinomial regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217639, 47)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pca = np.concatenate((np.asarray(df_copy.iloc[:, 0:3]), transformed), axis = 1)\n",
    "df_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133657, 46)\n",
      "(23587, 46)\n",
      "(32646, 46)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_pca[:, 1:], df_pca[:, 0], test_size=0.15, train_size=0.85, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15*0.85, train_size=0.85*0.85, shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8476922271186694\n",
      "0.8472887607580447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(multi_class=\"multinomial\", max_iter=100, solver = \"lbfgs\")\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_val, y_val))\n",
    "y_pred = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2587,   74,   39,   13,  110,    4,   64,   18,   75,   19],\n",
       "       [  29, 2805,  109,   25,   59,   23,   55,   29,   61,   35],\n",
       "       [  50,  119, 1870,   19,   72,   15,   73,   26,   51,    9],\n",
       "       [  57,   57,   30, 1947,   32,   41,   19,   10,   39,   13],\n",
       "       [  52,   76,   68,   19, 1865,   12,   81,    8,   66,   85],\n",
       "       [  10,   49,   25,   22,   12, 1714,   72,    7,   25,    6],\n",
       "       [  64,   92,   48,   15,   65,   12, 2104,    1,   30,   14],\n",
       "       [   2,   29,   10,    2,    8,    5,    4, 1872,    5,   33],\n",
       "       [ 129,  106,  111,   43,   68,   31,   63,   30, 1660,   58],\n",
       "       [  22,   34,    6,   19,   76,   19,    8,   19,   53, 1561]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So multinomial regression reached an accuracy of 85%, which is not very good by today's standards.  Althought it is still possible to fine tune this model to achieve better results, it might simply not improve much due to the limitation of the model itself. Next, I will use boosting methods to learn a more sophisitated model and see how much an ensemble model can improve the results of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paul\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8924785084208084\n",
      "0.8867596557425701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paul\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "boost_model = XGBClassifier(learning_rate=0.1, n_estimators=100, objective=\"multi:softprob\", booster=\"gbtree\", n_jobs=4)\n",
    "\n",
    "boost_model.fit(X_train, y_train)\n",
    "print(boost_model.score(X_train, y_train))\n",
    "print(boost_model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So both training and test accuracy scores improved, but not by much. I will use grid search cross validation to select the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=4,\n",
       "       param_grid={'max_depth': [3, 5], 'n_estimators': [100, 150], 'booster': ['gbtree', 'dart'], 'subsample': [0.6, 0.8, 1], 'reg_lambda': [0, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# It is possible to use a larger search space, but for now I will use this smaller search space to save time.\n",
    "\n",
    "params_dict = {\n",
    "    \"max_depth\": [3,5],\n",
    "    \"n_estimators\": [100, 150],\n",
    "    \"booster\": [\"gbtree\", \"dart\"],\n",
    "    \"subsample\": [0.6, 0.8, 1],\n",
    "    \"reg_lambda\": [0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "boost_model_cv = XGBClassifier()\n",
    "\n",
    "# Because GridSearch uses n-fold CV automatically, I create X_dev and y_dev with both training and validation sets\n",
    "\n",
    "X_dev = np.concatenate((X_train, X_val), axis = 0)\n",
    "y_dev = np.concatenate((y_train, y_val), axis = 0)\n",
    "\n",
    "cv = GridSearchCV(boost_model_cv, params_dict, scoring=\"accuracy\", n_jobs=4, cv=2)\n",
    "cv.fit(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'booster': 'dart', 'max_depth': 5, 'n_estimators': 150, 'reg_lambda': 0.01, 'subsample': 0.6}\n",
      "0.956608837221134\n"
     ]
    }
   ],
   "source": [
    "print(cv.best_params_)\n",
    "print(cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after grid search, accuracy improved to 95.6%, which is a pretty big margin. It seems that increasing number of estimators, max depths, decreasing regularization parameters, and choosing a smaller subsample size actually improved the model. It's still possible to fine tune the hyperparameters to achieve slightly better results, but this is pushing the boundary of what boosting models can do. Next, let's try a Convolutional Neural Network (CNN) with TensorFlow's high level estimator APIs. The neural network is constructed with a similar structure to LeNet-5 featured in [this tutorial](https://www.tensorflow.org/tutorials/layers).\n",
    "\n",
    "With CNN we only need the images and the labels. So I will perform a train-test split again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paul\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133657, 400)\n",
      "(133657,)\n",
      "(23587, 400)\n",
      "(32646, 400)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df_copy.values[:, 3:])\n",
    "\n",
    "#encoder = LabelBinarizer()\n",
    "#encoded = encoder.fit_transform(df_copy.values[:, 0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled, df_copy.values[:,0], test_size=0.15, train_size=0.85, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15*0.85, train_size=0.85*0.85, shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model function\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    with tf.device('/gpu:0'):\n",
    "        input_layer = tf.reshape(features[\"x\"], [-1, 20, 20, 1]) # images are 20x20\n",
    "\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs = input_layer,\n",
    "            filters = 32,\n",
    "            kernel_size = [3, 3],\n",
    "            padding = \"same\",\n",
    "            activation = tf.nn.relu\n",
    "        )\n",
    "\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs = pool1,\n",
    "            filters = 64,\n",
    "            kernel_size = [3, 3],\n",
    "            padding = \"same\",\n",
    "            activation = tf.nn.relu\n",
    "        )\n",
    "\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "        pool2_flat = tf.reshape(pool2, [-1, 5*5*64])\n",
    "\n",
    "        dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "        predictions = {\n",
    "            \"classes\": tf.argmax(input=logits, axis=1),\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "        }\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=loss, global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '.', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000028027A0ABA8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from .\\model.ckpt-32697\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 32698 into .\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.2258889228105545, step = 32697\n",
      "INFO:tensorflow:global_step/sec: 37.514\n",
      "INFO:tensorflow:loss = 0.24628768861293793, step = 32797 (2.667 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.2376\n",
      "INFO:tensorflow:loss = 0.23140285909175873, step = 32897 (2.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1826\n",
      "INFO:tensorflow:loss = 0.18678921461105347, step = 32997 (2.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.2619\n",
      "INFO:tensorflow:loss = 0.20423950254917145, step = 33097 (2.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1968\n",
      "INFO:tensorflow:loss = 0.1773894876241684, step = 33197 (2.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1554\n",
      "INFO:tensorflow:loss = 0.16653862595558167, step = 33297 (2.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.2619\n",
      "INFO:tensorflow:loss = 0.18774591386318207, step = 33397 (2.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1816\n",
      "INFO:tensorflow:loss = 0.137401282787323, step = 33497 (2.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1664\n",
      "INFO:tensorflow:loss = 0.11814052611589432, step = 33597 (2.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.4615\n",
      "INFO:tensorflow:loss = 0.19633130729198456, step = 33697 (2.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.0015\n",
      "INFO:tensorflow:loss = 0.27063775062561035, step = 33797 (2.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.9961\n",
      "INFO:tensorflow:loss = 0.16460643708705902, step = 33897 (2.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.0986\n",
      "INFO:tensorflow:loss = 0.20462846755981445, step = 33997 (2.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1696\n",
      "INFO:tensorflow:loss = 0.22753864526748657, step = 34097 (2.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.0512\n",
      "INFO:tensorflow:loss = 0.18041551113128662, step = 34197 (2.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.8925\n",
      "INFO:tensorflow:loss = 0.29147064685821533, step = 34297 (2.572 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.0172\n",
      "INFO:tensorflow:loss = 0.20700515806674957, step = 34397 (2.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.0597\n",
      "INFO:tensorflow:loss = 0.2182539403438568, step = 34497 (2.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1365\n",
      "INFO:tensorflow:loss = 0.26413819193840027, step = 34597 (2.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.0625\n",
      "INFO:tensorflow:loss = 0.1435977667570114, step = 34697 (2.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1389\n",
      "INFO:tensorflow:loss = 0.2040087729692459, step = 34797 (2.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1894\n",
      "INFO:tensorflow:loss = 0.10513819009065628, step = 34897 (2.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1949\n",
      "INFO:tensorflow:loss = 0.20722948014736176, step = 34997 (2.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.313\n",
      "INFO:tensorflow:loss = 0.05778295919299126, step = 35097 (2.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.2228\n",
      "INFO:tensorflow:loss = 0.09484263509511948, step = 35197 (2.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.2339\n",
      "INFO:tensorflow:loss = 0.3408163785934448, step = 35297 (2.549 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 35371 into .\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.11509912461042404.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x28027a0a278>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the estimator\n",
    "\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn = cnn_model_fn, model_dir = \".\")\n",
    "\n",
    "# Setup logging for predictions\n",
    "\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = {\"x\": X_train},\n",
    "    y = y_train,\n",
    "    batch_size = 100,\n",
    "    num_epochs = 2,\n",
    "    shuffle = True\n",
    ")\n",
    "mnist_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-01-02:56:39\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from .\\model.ckpt-35371\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-01-02:56:41\n",
      "INFO:tensorflow:Saving dict for global step 35371: accuracy = 0.95128673, global_step = 35371, loss = 0.17471893\n",
      "{'accuracy': 0.95128673, 'loss': 0.17471893, 'global_step': 35371}\n"
     ]
    }
   ],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_val},\n",
    "    y=y_val,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-01-02:56:44\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from .\\model.ckpt-35371\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-01-02:56:46\n",
      "INFO:tensorflow:Saving dict for global step 35371: accuracy = 0.95128673, global_step = 35371, loss = 0.17471893\n",
      "{'accuracy': 0.95128673, 'loss': 0.17471893, 'global_step': 35371}\n"
     ]
    }
   ],
   "source": [
    "test_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with CNN the accuracy is also around 95%. The model's performance improves as more training steps are performed. The use of GPU also significantly sped up the training process.  With XGBClassifier and GridSearchCV, it ended up taking more than 5 hours to search for the best model in a rather small search space, while with TensorFlow on a GTX1060 6GB each 20000 steps took about 5 minutes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
